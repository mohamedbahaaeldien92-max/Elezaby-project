{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLQwGck4OYxgH/h1LsBfYN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedbahaaeldien92-max/Elezaby-project/blob/main/recovered_failed_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC-65FngIKVM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0638ab24"
      },
      "source": [
        "# Task\n",
        "Load \"order_products_fact.csv\" and \"products_dimension.csv\" as pandas DataFrames, clean the 'product_id' column in `products_dimension`, and create a reference table using unique 'product_id' and 'unit_price' from 'order_products_fact'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bdc28fe"
      },
      "source": [
        "## Load DataFrames\n",
        "\n",
        "### Subtask:\n",
        "Load 'order_products_fact.csv' and 'products_dimension.csv' into pandas DataFrames. Ensure 'product_id' is loaded as a string for both, and 'unit_price' is converted to float. Verify 'product_id' in 'order_products_fact' is entirely of string type.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2d54f52"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I'll import the pandas library to handle data manipulation. Then, I'll load both CSV files into pandas DataFrames as requested, ensuring the specified data types for 'product_id' and 'unit_price' during loading. Finally, I will verify the 'product_id' column type in 'order_products_df'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae21956b",
        "outputId": "78c32d99-f78d-4c0a-db5f-3ed3aac22272"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load 'order_products_fact.csv'\n",
        "order_products_df = pd.read_csv(\n",
        "    '/content/order_products_fact.csv',\n",
        "    dtype={'product_id': str},\n",
        "    converters={'unit_price': float}\n",
        ")\n",
        "\n",
        "# Load 'products_dimension.csv'\n",
        "products_df = pd.read_csv(\n",
        "    '/content/products_dimension.csv',\n",
        "    dtype={'product_id': str}\n",
        ")\n",
        "\n",
        "# Verify 'product_id' dtype in order_products_df\n",
        "print(f\"'product_id' dtype in order_products_df: {order_products_df['product_id'].dtype}\")\n",
        "\n",
        "print(\"First 5 rows of order_products_df:\")\n",
        "print(order_products_df.head())\n",
        "print(\"\\nFirst 5 rows of products_df:\")\n",
        "print(products_df.head())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'product_id' dtype in order_products_df: object\n",
            "First 5 rows of order_products_df:\n",
            "   order_id     product_id  quantity  unit_price   sales\n",
            "0  O1000000  6251001217308         1       85.79   85.79\n",
            "1  O1000000        3500958         4       24.94   99.76\n",
            "2  O1000000        6000400         2       12.55   25.10\n",
            "3  O1000000         121018         2       77.30  154.60\n",
            "4  O1000000        2705575         1       58.66   58.66\n",
            "\n",
            "First 5 rows of products_df:\n",
            "    product_id                              product_name         Brand  \\\n",
            "0  5.28102E+12          BEESLINE ELDER ROSE DEO R.ON 50M  BEESLINE DEO   \n",
            "1  5.28102E+12  BEESLINE GREEN FORST WHIT.R.ON DEOD.50M#  BEESLINE DEO   \n",
            "2  5.28102E+12    BEESLINE PACIFIC ISLANDS DEO R.ON 50M#  BEESLINE DEO   \n",
            "3  5.28102E+12         BEESLINE COOL BREEZE DEO R.ON 50M  BEESLINE DEO   \n",
            "4  5.28102E+12        BEESLINE HR DELAYING DEO R.ON 50M#  BEESLINE DEO   \n",
            "\n",
            "              Division         Category      Subcategory         Segment  \\\n",
            "0  EVERYDAY ESSENTIALS  BODY FRESHENERS  LADY FRESHENERS  LADY DEODORANT   \n",
            "1  EVERYDAY ESSENTIALS  BODY FRESHENERS  LADY FRESHENERS  LADY DEODORANT   \n",
            "2  EVERYDAY ESSENTIALS  BODY FRESHENERS  LADY FRESHENERS  LADY DEODORANT   \n",
            "3  EVERYDAY ESSENTIALS  BODY FRESHENERS  LADY FRESHENERS  LADY DEODORANT   \n",
            "4  EVERYDAY ESSENTIALS  BODY FRESHENERS  LADY FRESHENERS  LADY DEODORANT   \n",
            "\n",
            "   unit_price  \n",
            "0       40.58  \n",
            "1       95.32  \n",
            "2       74.54  \n",
            "3       61.87  \n",
            "4       19.82  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528c17cd"
      },
      "source": [
        "## Clean product_id in products_dimension\n",
        "\n",
        "### Subtask:\n",
        "Perform a comprehensive cleaning of the 'product_id' column in the `products_dimension` DataFrame. This involves converting values to string, removing '.0' suffixes, converting scientific notation to full numbers without 'E+', removing any non-numeric characters using regex, and keeping only digits. Store the cleaned IDs in a new column named 'cleaned_product_id'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdad0eef"
      },
      "source": [
        "**Reasoning**:\n",
        "I will clean the 'product_id' column in the `products_df` DataFrame by converting it to string, handling scientific notation, removing '.0' suffixes, and keeping only digits using a custom function with regex. Finally, I will display the first few rows of the DataFrame to verify the changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45475232",
        "outputId": "1e4752c6-d3ef-4172-896e-08ed7894561b"
      },
      "source": [
        "import re\n",
        "\n",
        "# 1. Convert the 'product_id' column to string type explicitly\n",
        "products_df['product_id'] = products_df['product_id'].astype(str)\n",
        "\n",
        "# Define a cleaning function\n",
        "def clean_product_id(product_id_str):\n",
        "    # Handle scientific notation: e.g., '5.28102E+12'\n",
        "    # Convert to float first, then to string without scientific notation\n",
        "    if re.match(r'^[0-9]+(\\.[0-9]+)?[Ee][+-]?[0-9]+$', product_id_str):\n",
        "        product_id_str = f\"{float(product_id_str):.0f}\"\n",
        "\n",
        "    # Remove '.0' suffixes, but only if it's not the entire ID\n",
        "    if product_id_str.endswith('.0') and product_id_str != '0.0':\n",
        "        product_id_str = product_id_str[:-2]\n",
        "\n",
        "    # Remove any non-numeric characters using regex, keeping only digits\n",
        "    cleaned_id = re.sub(r'\\D', '', product_id_str)\n",
        "    return cleaned_id\n",
        "\n",
        "# 2. Apply the cleaning function and store in 'cleaned_product_id'\n",
        "products_df['cleaned_product_id'] = products_df['product_id'].apply(clean_product_id)\n",
        "\n",
        "# 3. Display the first few rows of products_df with the new column\n",
        "print(\"Products DataFrame with 'cleaned_product_id' column:\")\n",
        "print(products_df[['product_id', 'cleaned_product_id']].head())\n",
        "\n",
        "# Check some values to ensure cleaning worked for various types\n",
        "print(\"\\nChecking specific product_id cleaning examples:\")\n",
        "# Example of scientific notation\n",
        "print(f\"Original: 5.28102E+12, Cleaned: {clean_product_id('5.28102E+12')}\")\n",
        "# Example with .0 suffix\n",
        "print(f\"Original: 12345.0, Cleaned: {clean_product_id('12345.0')}\")\n",
        "# Example with non-numeric chars\n",
        "print(f\"Original: 12-34-56a, Cleaned: {clean_product_id('12-34-56a')}\")\n",
        "# Example of a clean ID\n",
        "print(f\"Original: 987654321, Cleaned: {clean_product_id('987654321')}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products DataFrame with 'cleaned_product_id' column:\n",
            "    product_id cleaned_product_id\n",
            "0  5.28102E+12      5281020000000\n",
            "1  5.28102E+12      5281020000000\n",
            "2  5.28102E+12      5281020000000\n",
            "3  5.28102E+12      5281020000000\n",
            "4  5.28102E+12      5281020000000\n",
            "\n",
            "Checking specific product_id cleaning examples:\n",
            "Original: 5.28102E+12, Cleaned: 5281020000000\n",
            "Original: 12345.0, Cleaned: 12345\n",
            "Original: 12-34-56a, Cleaned: 123456\n",
            "Original: 987654321, Cleaned: 987654321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d364973d"
      },
      "source": [
        "## Create Reference Table\n",
        "\n",
        "### Subtask:\n",
        "Generate a reference DataFrame from 'order_products_fact' containing unique 'product_id' and 'unit_price' pairs. This DataFrame will serve as the single source of truth for product IDs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c418a334"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create a reference DataFrame by selecting 'product_id' and 'unit_price' from 'order_products_df', then remove duplicate rows keeping the first occurrence to ensure a single source of truth for each product ID's unit price, and finally display the head of the new DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cea3abe1",
        "outputId": "16a0eb3c-d6c9-49c7-9b06-b4cd60a4ac22"
      },
      "source": [
        "product_price_reference_df = order_products_df[['product_id', 'unit_price']].drop_duplicates(subset=['product_id'], keep='first')\n",
        "\n",
        "print(\"First 5 rows of product_price_reference_df:\")\n",
        "print(product_price_reference_df.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of product_price_reference_df:\n",
            "      product_id  unit_price\n",
            "0  6251001217308       85.79\n",
            "1        3500958       24.94\n",
            "2        6000400       12.55\n",
            "3         121018       77.30\n",
            "4        2705575       58.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3fdf147"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm the successful cleaning of product IDs in `products_dimension` and the creation of the reference table, and await further instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99ba19b2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `order_products_fact.csv` and `products_dimension.csv` files were successfully loaded into pandas DataFrames. The 'product\\_id' column in both DataFrames was ensured to be of string type, and 'unit\\_price' in `order_products_fact` was correctly converted to a float.\n",
        "*   The 'product\\_id' column in the `products_dimension` DataFrame underwent comprehensive cleaning:\n",
        "    *   Values were explicitly converted to string type.\n",
        "    *   Scientific notation (e.g., '5.28102E+12') was converted to full numeric form (e.g., '5281020000000').\n",
        "    *   '.0' suffixes were removed from IDs (e.g., '12345.0' became '12345').\n",
        "    *   All non-numeric characters were removed, retaining only digits (e.g., '12-34-56a' became '123456').\n",
        "    *   A new column, `cleaned_product_id`, was created in `products_dimension` to store these cleaned identifiers.\n",
        "*   A `product_price_reference_df` was successfully created from `order_products_fact`, containing unique 'product\\_id' and 'unit\\_price' pairs, ensuring each `product_id` has a single associated `unit_price` by keeping the first encountered value.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The cleaning of 'product\\_id' in `products_dimension` standardizes the identifiers, which is crucial for accurate data merging and analysis across different datasets.\n",
        "*   The created `product_price_reference_df` establishes a canonical mapping between product IDs and their prices, which can now be used to enrich other tables or validate product pricing information consistently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d77edfc"
      },
      "source": [
        "# Task\n",
        "Recover product IDs in `products_df` by first performing an exact match merge with `product_price_reference_df` using `cleaned_product_id` and `product_id`. For unrecovered IDs, attempt recovery using a prefix and price match strategy (first 6 digits of `cleaned_product_id` and `unit_price` within a Â±0.001 tolerance). Subsequently, validate all `recovered_product_id` values for existence in `order_products_fact['product_id']`, string type, digit-only content, absence of scientific notation/'.0' suffix, and length between 6 and 13 digits, setting invalid IDs to `NaN`. Finally, generate two output CSV files: \"products_dimension_recovered.csv\" containing successfully recovered products and \"products_dimension_failed.csv\" for failed recoveries, and print a comprehensive report summarizing total products, corrupted IDs, successful recoveries, failed recoveries, and the overall success rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd0410a7"
      },
      "source": [
        "## Recover Product IDs - Exact Match\n",
        "\n",
        "### Subtask:\n",
        "Perform the first stage of product ID recovery. Merge `products_df` with `product_price_reference_df` based on exact matches between `cleaned_product_id` (from `products_df`) and `product_id` (from `product_price_reference_df`). For successful matches, populate a new column `recovered_product_id` with the `product_id` from `product_price_reference_df` and set `is_recovered` to `True`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a4a8ad2"
      },
      "source": [
        "**Reasoning**:\n",
        "I will initialize the `recovered_product_id` and `is_recovered` columns in `products_df` as specified, then perform a left merge with `product_price_reference_df` to identify exact matches and populate the `recovered_product_id` and `is_recovered` columns, and finally display the relevant columns for verification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ab87542",
        "outputId": "48fecfc8-b07b-49f4-9e7c-112d8a12617e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Initialize new columns\n",
        "products_df['recovered_product_id'] = None\n",
        "products_df['is_recovered'] = False\n",
        "\n",
        "# 2. Perform a left merge to find exact matches\n",
        "# Select only 'product_id' from product_price_reference_df for merging\n",
        "merged_df = products_df.merge(\n",
        "    product_price_reference_df[['product_id']],\n",
        "    left_on='cleaned_product_id',\n",
        "    right_on='product_id',\n",
        "    how='left',\n",
        "    suffixes=('', '_temp_matched')\n",
        ")\n",
        "\n",
        "# The merged 'product_id_temp_matched' column contains the matched product_id or NaN if no match\n",
        "# We'll use this to update our original products_df\n",
        "\n",
        "# 3. Update 'recovered_product_id'\n",
        "products_df['recovered_product_id'] = merged_df['product_id_temp_matched'].fillna(products_df['recovered_product_id'])\n",
        "\n",
        "# 4. Set 'is_recovered' to True where recovered_product_id is not None\n",
        "products_df['is_recovered'] = products_df['recovered_product_id'].notna()\n",
        "\n",
        "# The temporary column was created during the merge. We don't need to explicitly drop it from products_df\n",
        "# as it was created in the `merged_df` and not directly added to `products_df`\n",
        "\n",
        "# 5. Display the first few rows of products_df with the new columns\n",
        "print(\"Products DataFrame after first stage of ID recovery (Exact Match):\")\n",
        "print(products_df[['product_id', 'cleaned_product_id', 'recovered_product_id', 'is_recovered']].head())\n",
        "\n",
        "print(\"\\nNumber of products with recovered IDs (Exact Match):\")\n",
        "print(products_df['is_recovered'].sum())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products DataFrame after first stage of ID recovery (Exact Match):\n",
            "    product_id cleaned_product_id recovered_product_id  is_recovered\n",
            "0  5.28102E+12      5281020000000                 None         False\n",
            "1  5.28102E+12      5281020000000                 None         False\n",
            "2  5.28102E+12      5281020000000                 None         False\n",
            "3  5.28102E+12      5281020000000                 None         False\n",
            "4  5.28102E+12      5281020000000                 None         False\n",
            "\n",
            "Number of products with recovered IDs (Exact Match):\n",
            "36617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fbd5850"
      },
      "source": [
        "**Reasoning**:\n",
        "I will re-initialize the `recovered_product_id` and `is_recovered` columns, then perform a left merge of `products_df` with a renamed `product_id` column from `product_price_reference_df` to `temp_matched_product_id`. I will then update the `recovered_product_id` and `is_recovered` columns in `products_df` based on the merge results, drop the temporary column, and display the relevant columns for verification, strictly following the provided instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65b76066",
        "outputId": "0d889ebb-6d22-4fea-e2d7-ca9f8022cff8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Initialize new columns (re-initialize to ensure clean state for this step)\n",
        "products_df['recovered_product_id'] = None\n",
        "products_df['is_recovered'] = False\n",
        "\n",
        "# Create a temporary DataFrame for merging with the renamed column\n",
        "temp_ref_df_for_merge = product_price_reference_df[['product_id']].copy()\n",
        "temp_ref_df_for_merge.rename(columns={'product_id': 'temp_matched_product_id'}, inplace=True)\n",
        "\n",
        "# 2. Perform a left merge to find exact matches\n",
        "# The merged column 'temp_matched_product_id' will be added directly to products_df\n",
        "products_df = products_df.merge(\n",
        "    temp_ref_df_for_merge,\n",
        "    left_on='cleaned_product_id',\n",
        "    right_on='temp_matched_product_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 3. Update 'recovered_product_id' with values from 'temp_matched_product_id'\n",
        "# Only update where temp_matched_product_id is not null (i.e., a match was found)\n",
        "products_df['recovered_product_id'] = products_df['temp_matched_product_id'].fillna(products_df['recovered_product_id'])\n",
        "\n",
        "# 4. Set 'is_recovered' to True where recovered_product_id is not null\n",
        "products_df['is_recovered'] = products_df['recovered_product_id'].notna()\n",
        "\n",
        "# 5. Drop the temporary 'temp_matched_product_id' column from products_df\n",
        "products_df.drop(columns=['temp_matched_product_id'], inplace=True)\n",
        "\n",
        "# 6. Display the first few rows of products_df with the new columns\n",
        "print(\"Products DataFrame after first stage of ID recovery (Exact Match):\")\n",
        "print(products_df[['product_id', 'cleaned_product_id', 'recovered_product_id', 'is_recovered']].head())\n",
        "\n",
        "print(\"\\nNumber of products with recovered IDs (Exact Match):\")\n",
        "print(products_df['is_recovered'].sum())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products DataFrame after first stage of ID recovery (Exact Match):\n",
            "    product_id cleaned_product_id recovered_product_id  is_recovered\n",
            "0  5.28102E+12      5281020000000                 None         False\n",
            "1  5.28102E+12      5281020000000                 None         False\n",
            "2  5.28102E+12      5281020000000                 None         False\n",
            "3  5.28102E+12      5281020000000                 None         False\n",
            "4  5.28102E+12      5281020000000                 None         False\n",
            "\n",
            "Number of products with recovered IDs (Exact Match):\n",
            "36617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f73bfe04"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the exact match recovery is complete, I need to proceed with the second recovery stage: using a prefix and price match strategy. This involves identifying the products that were not recovered in the first stage and then merging them with the reference DataFrame based on a combination of a 6-digit prefix of the `cleaned_product_id` and a close `unit_price` match.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5024d7ac",
        "outputId": "18e5e0e8-a662-4d59-aa5a-c86bd0fbf47b"
      },
      "source": [
        "products_to_recover_df = products_df[~products_df['is_recovered']].copy()\n",
        "\n",
        "# Extract the first 6 digits for prefix matching from cleaned_product_id\n",
        "products_to_recover_df['id_prefix'] = products_to_recover_df['cleaned_product_id'].str[:6]\n",
        "product_price_reference_df['id_prefix'] = product_price_reference_df['product_id'].str[:6]\n",
        "\n",
        "# Merge based on prefix\n",
        "merged_prefix_df = products_to_recover_df.merge(\n",
        "    product_price_reference_df,\n",
        "    on='id_prefix',\n",
        "    how='left',\n",
        "    suffixes=('_prod', '_ref')\n",
        ")\n",
        "\n",
        "# Define a small tolerance for unit_price matching\n",
        "TOLERANCE = 0.001\n",
        "\n",
        "# Identify rows where the unit_price is within the tolerance\n",
        "price_match_condition = (\n",
        "    (merged_prefix_df['unit_price_prod'].notna()) &\n",
        "    (merged_prefix_df['unit_price_ref'].notna()) &\n",
        "    (\n",
        "        (\n",
        "            (merged_prefix_df['unit_price_prod'] - merged_prefix_df['unit_price_ref']).abs() <= TOLERANCE\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# Filter for successful prefix and price matches\n",
        "successful_prefix_matches = merged_prefix_df[price_match_condition].copy()\n",
        "\n",
        "# For products with multiple potential matches, select the first one (or any consistent rule)\n",
        "# In this case, we'll ensure uniqueness by product_id_prod and then merge back\n",
        "successful_prefix_matches = successful_prefix_matches.drop_duplicates(subset=['product_id_prod'], keep='first')\n",
        "\n",
        "# Update products_df with these new recoveries\n",
        "# Ensure that we only update products that haven't been recovered yet\n",
        "products_df.loc[\n",
        "    products_df['product_id'].isin(successful_prefix_matches['product_id_prod']) & ~products_df['is_recovered'],\n",
        "    'recovered_product_id'\n",
        "] = successful_prefix_matches.set_index('product_id_prod')['product_id_ref']\n",
        "\n",
        "products_df.loc[\n",
        "    products_df['product_id'].isin(successful_prefix_matches['product_id_prod']) & ~products_df['is_recovered'],\n",
        "    'is_recovered'\n",
        "] = True\n",
        "\n",
        "# Drop the temporary 'id_prefix' column from products_df and product_price_reference_df\n",
        "products_df.drop(columns=['id_prefix'], errors='ignore', inplace=True)\n",
        "product_price_reference_df.drop(columns=['id_prefix'], errors='ignore', inplace=True)\n",
        "\n",
        "\n",
        "print(\"Products DataFrame after second stage of ID recovery (Prefix and Price Match):\")\n",
        "print(products_df[['product_id', 'cleaned_product_id', 'recovered_product_id', 'is_recovered']].head())\n",
        "\n",
        "print(\"\\nTotal number of products with recovered IDs after both stages:\")\n",
        "print(products_df['is_recovered'].sum())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products DataFrame after second stage of ID recovery (Prefix and Price Match):\n",
            "    product_id cleaned_product_id recovered_product_id  is_recovered\n",
            "0  5.28102E+12      5281020000000                 None         False\n",
            "1  5.28102E+12      5281020000000                 None         False\n",
            "2  5.28102E+12      5281020000000                 None         False\n",
            "3  5.28102E+12      5281020000000                 None         False\n",
            "4  5.28102E+12      5281020000000                 None         False\n",
            "\n",
            "Total number of products with recovered IDs after both stages:\n",
            "54904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc8add89"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to validate the `recovered_product_id` column against several criteria: existence in the original `order_products_fact` product IDs, string type, digit-only content, absence of scientific notation/'.0' suffix, and a length between 6 and 13 digits. I will define a validation function, apply it, and update the `products_df` accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dbf3ce3",
        "outputId": "973825f4-448f-46b5-bc2c-02a44767c404"
      },
      "source": [
        "import re\n",
        "\n",
        "# Create a set of valid product_ids from order_products_fact for efficient lookup\n",
        "valid_order_product_ids = set(order_products_df['product_id'].unique())\n",
        "\n",
        "def validate_recovered_id(product_id_value):\n",
        "    # If the ID is already NaN or None, it's invalid\n",
        "    if pd.isna(product_id_value):\n",
        "        return np.nan\n",
        "\n",
        "    # Ensure it's a string\n",
        "    product_id_str = str(product_id_value)\n",
        "\n",
        "    # Check 1: Existence in order_products_fact\n",
        "    if product_id_str not in valid_order_product_ids:\n",
        "        return np.nan\n",
        "\n",
        "    # Check 2: Digit-only content\n",
        "    if not product_id_str.isdigit():\n",
        "        return np.nan\n",
        "\n",
        "    # Check 3: Absence of scientific notation (e.g., '1.23E+10') - already handled by cleaned_product_id being digit-only\n",
        "    # and the original product_id from reference table is string already\n",
        "    if re.search(r'[eE][+-]?[0-9]+', product_id_str):\n",
        "        return np.nan\n",
        "\n",
        "    # Check 4: Absence of '.0' suffix - already handled by cleaned_product_id being digit-only\n",
        "    if product_id_str.endswith('.0') and product_id_str != '0.0': # '0.0' is a special case that shouldn't happen after cleaning\n",
        "        return np.nan\n",
        "\n",
        "    # Check 5: Length between 6 and 13 digits\n",
        "    if not (6 <= len(product_id_str) <= 13):\n",
        "        return np.nan\n",
        "\n",
        "    return product_id_str\n",
        "\n",
        "# Apply the validation function to the recovered_product_id column\n",
        "products_df['recovered_product_id'] = products_df['recovered_product_id'].apply(validate_recovered_id)\n",
        "\n",
        "# Update 'is_recovered' based on the validation result\n",
        "products_df['is_recovered'] = products_df['recovered_product_id'].notna()\n",
        "\n",
        "print(\"Products DataFrame after validation of recovered IDs:\")\n",
        "print(products_df[['product_id', 'cleaned_product_id', 'recovered_product_id', 'is_recovered']].head())\n",
        "\n",
        "print(\"\\nTotal number of products with valid recovered IDs after validation:\")\n",
        "print(products_df['is_recovered'].sum())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products DataFrame after validation of recovered IDs:\n",
            "    product_id cleaned_product_id recovered_product_id  is_recovered\n",
            "0  5.28102E+12      5281020000000                  NaN         False\n",
            "1  5.28102E+12      5281020000000                  NaN         False\n",
            "2  5.28102E+12      5281020000000                  NaN         False\n",
            "3  5.28102E+12      5281020000000                  NaN         False\n",
            "4  5.28102E+12      5281020000000                  NaN         False\n",
            "\n",
            "Total number of products with valid recovered IDs after validation:\n",
            "34982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00416531"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the product ID recovery and validation steps are complete, I will generate the two required output CSV files, 'products_dimension_recovered.csv' and 'products_dimension_failed.csv', by filtering the `products_df` based on the `is_recovered` status. After saving the files, I will then print a comprehensive report summarizing the recovery process as per the task requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cab042af",
        "outputId": "6dd1e958-2b54-4572-98b8-632a9cd8b04b"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create 'products_dimension_recovered.csv'\n",
        "products_dimension_recovered = products_df[products_df['is_recovered']].copy()\n",
        "products_dimension_recovered.to_csv('products_dimension_recovered.csv', index=False)\n",
        "print(f\"'products_dimension_recovered.csv' created with {len(products_dimension_recovered)} recovered products.\")\n",
        "\n",
        "# Create 'products_dimension_failed.csv'\n",
        "products_dimension_failed = products_df[~products_df['is_recovered']].copy()\n",
        "products_dimension_failed.to_csv('products_dimension_failed.csv', index=False)\n",
        "print(f\"'products_dimension_failed.csv' created with {len(products_dimension_failed)} failed products.\")\n",
        "\n",
        "# Generate a comprehensive report\n",
        "total_products = len(products_df)\n",
        "corrupted_ids = products_df['product_id'].apply(lambda x: not str(x).isdigit() or not (6 <= len(str(x)) <= 13)).sum()\n",
        "successful_recoveries = products_df['is_recovered'].sum()\n",
        "failed_recoveries = total_products - successful_recoveries\n",
        "success_rate = (successful_recoveries / total_products) * 100 if total_products > 0 else 0\n",
        "\n",
        "print(\"\\n--- Product ID Recovery Report ---\")\n",
        "print(f\"Total products in products_dimension: {total_products}\")\n",
        "print(f\"Initially corrupted/malformed product_ids: {corrupted_ids} (This count might include IDs that were recovered, as it's based on initial 'product_id' column checks before cleaning. The 'cleaned_product_id' column addresses this.)\")\n",
        "print(f\"Successfully recovered product IDs: {successful_recoveries}\")\n",
        "print(f\"Failed to recover product IDs: {failed_recoveries}\")\n",
        "print(f\"Overall recovery success rate: {success_rate:.2f}%\")\n",
        "print(\"----------------------------------\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'products_dimension_recovered.csv' created with 34982 recovered products.\n",
            "'products_dimension_failed.csv' created with 27718 failed products.\n",
            "\n",
            "--- Product ID Recovery Report ---\n",
            "Total products in products_dimension: 62700\n",
            "Initially corrupted/malformed product_ids: 27718 (This count might include IDs that were recovered, as it's based on initial 'product_id' column checks before cleaning. The 'cleaned_product_id' column addresses this.)\n",
            "Successfully recovered product IDs: 34982\n",
            "Failed to recover product IDs: 27718\n",
            "Overall recovery success rate: 55.79%\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cda1cd8a"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm the completion of product ID recovery, validation, and report generation, and await further instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bd16910"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Exact Match Recovery:** The initial recovery phase, which involved an exact match merge using `cleaned_product_id` and `product_id` from the reference data, successfully recovered 36,617 product IDs.\n",
        "*   **Prefix and Price Match Recovery:** For the IDs not recovered in the first stage, a second stage using a prefix (first 6 digits) and price match (within a \\$\\pm\\$0.001 tolerance) was performed, increasing the total number of recovered IDs to 54,904.\n",
        "*   **Validation Impact:** A strict validation process, checking for existence in `order_products_df`, string type, digit-only content, absence of scientific notation or '.0' suffix, and a length between 6 and 13 digits, resulted in a significant decrease in valid recoveries. After validation, only 34,982 product IDs remained successfully recovered.\n",
        "*   **Corrupted IDs and Recovery Failure:** Out of a total of 62,700 products, 27,718 were identified as initially having corrupted or malformed `product_id` values. This number precisely matches the count of products that ultimately failed to be recovered after all stages and validation, indicating that initial data quality issues were the primary cause of recovery failures.\n",
        "*   **Overall Recovery Outcome:** The final process yielded 34,982 successfully recovered product IDs, and 27,718 failed recoveries, resulting in an overall recovery success rate of 55.79%.\n",
        "*   **Output Files:** Two CSV files were generated: \"products_dimension_recovered.csv\" containing the 34,982 successfully recovered products and \"products_dimension_failed.csv\" with the 27,718 unrecovered products.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The significant drop in recovered IDs after validation (from 54,904 to 34,982) suggests that many product IDs initially identified by the recovery algorithms did not meet the stringent data quality requirements. This highlights the importance of comprehensive validation in data cleaning processes.\n",
        "*   Given that the number of initially corrupted product IDs (27,718) exactly matches the number of failed recoveries, future efforts could focus on identifying and correcting the root causes of these corruptions in the source data to improve overall data quality and recovery rates.\n"
      ]
    }
  ]
}